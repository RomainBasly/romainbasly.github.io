<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="description" content="RB's blog"><meta name="viewport" content="width=device-width"><link rel="icon" type="image/svg+xml" href="/favicon.png"><meta name="generator" content="Astro v4.16.14"><title>Automating Redundant Work Tasks with a Local AI Model - Setup</title><link rel="stylesheet" href="/_astro/about.DnqFRiBh.css">
<style>.post-content[data-astro-cid-gjtny2mx]{display:flex;flex-direction:column;gap:20px;position:relative;padding:200px 150px;@media screen and (max-width: 1200px){padding:200px 150px}@media screen and (max-width: 900px){padding:200px 120px}@media screen and (max-width: 650px){padding:200px 50px}@media screen and (max-width: 500px){padding:120px 10px}@media screen and (max-width: 360px){padding:120px 10px 200px}}.post-title[data-astro-cid-gjtny2mx]{color:#ff5c39;font-weight:700;font-size:4rem;font-family:Cairo,sans-serif;margin-bottom:8px;margin-block-start:0px;line-height:1.2;@media screen and (max-width: 1200px){font-size:4rem}@media screen and (max-width: 900px){font-size:3rem}@media screen and (max-width: 650px){font-size:2.8rem}@media screen and (max-width: 500px){font-size:2.5rem}@media screen and (max-width: 360px){font-size:2rem}}.content[data-astro-cid-gjtny2mx]{z-index:1;font-family:Mukta,sans-serif}.author-date[data-astro-cid-gjtny2mx]{margin:0;color:#6c757d}.tags-container[data-astro-cid-gjtny2mx]{display:flex;gap:6px;align-items:center;flex-wrap:wrap}.tag[data-astro-cid-gjtny2mx]{background:inherit;font-style:italic;border:1px solid white;border-radius:24px;color:#fff;padding:8px 16px;user-select:none}.tag-link[data-astro-cid-gjtny2mx]{text-decoration:none}tag-separator[data-astro-cid-gjtny2mx]:last-of-type{display:none}.picture-blog[data-astro-cid-gjtny2mx]{display:flex;align-items:center;justify-content:center;border-radius:50%;width:52px;height:50px;overflow:hidden}.picture-avatar[data-astro-cid-gjtny2mx]{width:100%;object-fit:contain}.avatar[data-astro-cid-gjtny2mx]{display:flex;gap:10px;align-items:center}.head-article[data-astro-cid-gjtny2mx]{display:flex;flex-direction:column;gap:10px;margin-bottom:20px}table[data-astro-cid-gjtny2mx]{width:100%;border:1px solid white;border-collapse:collapse;margin-bottom:1.5rem}th[data-astro-cid-gjtny2mx]{border:1px solid white;padding:.75rem;text-align:left;background-color:gray}td[data-astro-cid-gjtny2mx]{border:1px solid white;padding:.75rem}caption[data-astro-cid-gjtny2mx]{caption-side:bottom;text-align:center;padding-top:.5rem;color:#6c757d;font-style:italic}.translation-link[data-astro-cid-gjtny2mx]{display:flex;gap:10px;align-items:center;font-size:18px;text-decoration:none;cursor:pointer;border-radius:4px;font-style:italic;padding:4px}.translation-section[data-astro-cid-gjtny2mx]{display:flex;flex-direction:row;gap:10px}.translation-text[data-astro-cid-gjtny2mx]{display:flex;gap:8px;align-items:center}.icon[data-astro-cid-gjtny2mx]{display:flex;width:100%}
</style></head> <body>  <article class="post-content" data-astro-cid-gjtny2mx> <section class="head-article" data-astro-cid-gjtny2mx> <h1 class="post-title" data-astro-cid-gjtny2mx>Automating Redundant Work Tasks with a Local AI Model - Setup</h1> <div class="avatar" data-astro-cid-gjtny2mx> <div class="picture-blog" data-astro-cid-gjtny2mx> <img src="/_astro/RB-photo.BKGyKm44.png" alt="Picture of the author" class="picture-avatar" data-astro-cid-gjtny2mx> </div> <div class="author-date" data-astro-cid-gjtny2mx> By Romain Basly on February 27, 2025 </div> </div> <div class="tags-container" data-astro-cid-gjtny2mx>
Tags:
<button class="tag" data-astro-cid-gjtny2mx> <a href="/tags/AI" class="tag-link" data-astro-cid-gjtny2mx> AI </a> </button><button class="tag" data-astro-cid-gjtny2mx> <a href="/tags/Mistral" class="tag-link" data-astro-cid-gjtny2mx> Mistral </a> </button><button class="tag" data-astro-cid-gjtny2mx> <a href="/tags/HuggingFace" class="tag-link" data-astro-cid-gjtny2mx> HuggingFace </a> </button><button class="tag" data-astro-cid-gjtny2mx> <a href="/tags/Automatization" class="tag-link" data-astro-cid-gjtny2mx> Automatization </a> </button><button class="tag" data-astro-cid-gjtny2mx> <a href="/tags/Better-Faster-Stronger" class="tag-link" data-astro-cid-gjtny2mx> Better-Faster-Stronger </a> </button> </div> </section> <section class="translation-section" data-astro-cid-gjtny2mx> <a href="/posts/fr/7" class="translation-link" data-astro-cid-gjtny2mx> <svg width="1em" height="1em" data-astro-cid-gjtny2mx data-icon="maki:arrow">   <symbol id="ai:maki:arrow" viewBox="0 0 15 15"><path fill="currentColor" d="M8.293 2.293a1 1 0 0 1 1.414 0l4.5 4.5a1 1 0 0 1 0 1.414l-4.5 4.5a1 1 0 0 1-1.414-1.414L11 8.5H1.5a1 1 0 0 1 0-2H11L8.293 3.707a1 1 0 0 1 0-1.414"/></symbol><use href="#ai:maki:arrow"></use>  </svg> <div class="translation-text" data-astro-cid-gjtny2mx> <div class="content" data-astro-cid-gjtny2mx>
Cet article est aussi disponible en Français
</div> <svg width="1em" height="1em" class="icon-svg" data-astro-cid-gjtny2mx data-icon="noto:baguette-bread">   <symbol id="ai:noto:baguette-bread" viewBox="0 0 128 128"><path fill="#E38413" d="m98.3 39.81l-55.45 60.78l-38.39 14.72s-1.48 3.31 1.58 6c2.57 2.27 6.25 1.14 7.84.57s4.39.16 7.38.23c2.3.05 7.72-.91 11.13-2.5s10-5.34 21.7-16.24s26.04-25.09 31.57-31.25C96.19 60.39 97.76 58 107 45.39c7.22-9.86 11.39-18.15 13.32-22.35s4.66-9.77 4.09-13.06S121 6.11 121 6.11z"/><radialGradient id="notoBaguetteBread0" cx="47.518" cy="49.924" r="43.766" gradientTransform="matrix(.7047 .7095 -1.77 1.7578 102.401 -71.55)" gradientUnits="userSpaceOnUse"><stop offset=".445" stop-color="#DB8316"/><stop offset=".539" stop-color="#DF8A1B"/><stop offset=".671" stop-color="#E99C2A"/><stop offset=".79" stop-color="#F6B23B"/></radialGradient><path fill="url(#notoBaguetteBread0)" d="M83.47 18.47c-7.67 5.49-19.39 17.04-33.3 32C33.25 68.66 15.7 89.71 12.28 95.9c-5.2 9.41-4.15 11.42-5.5 14.26s-3.92 5.3-2.09 8.6c1.46 2.65 5.49 1.36 7.74.51c1.79-.68 4.58-.27 8.11-.08c6.11.32 13.48-2.43 22.05-9.34S73.62 80.31 89.1 63.47s29.72-41.93 32.3-47.24s3.02-6.16 3.02-7.08c0-.93-1.63-5.35-11.96-3.91s-21.45 7.83-28.99 13.23"/><radialGradient id="notoBaguetteBread1" cx="99.251" cy="16.863" r="9.919" gradientTransform="matrix(.8 -.6 .3781 .5041 13.475 67.913)" gradientUnits="userSpaceOnUse"><stop offset=".029" stop-color="#FBD8A9"/><stop offset="1" stop-color="#FBD8A9" stop-opacity="0"/></radialGradient><path fill="url(#notoBaguetteBread1)" d="M102.2 8.52c-2.83.25-8.96 4.19-10.4 5.76c-4.59 5-1.41 10.03 2.5 10.16c7.38.24 13.31-4.59 14.59-9.13c1.61-5.74-2.67-7.15-6.69-6.79"/><radialGradient id="notoBaguetteBread2" cx="76.352" cy="40.241" r="13.416" gradientTransform="matrix(.9934 -.5282 .4699 .6456 -18.34 54.592)" gradientUnits="userSpaceOnUse"><stop offset=".097" stop-color="#FDE5BF"/><stop offset=".37" stop-color="#FCE1B9" stop-opacity=".742"/><stop offset=".716" stop-color="#FAD8A9" stop-opacity=".416"/><stop offset=".843" stop-color="#FCE0B6" stop-opacity=".211"/><stop offset=".974" stop-color="#FDE5BF" stop-opacity="0"/></radialGradient><path fill="url(#notoBaguetteBread2)" d="M75.15 28.12c-3.34 1.22-9.05 6.9-10.73 8.5c-4.01 3.84-6.83 15.41 5.78 16.1c10.29.56 22.76-9.43 20.67-18.89c-1.48-6.76-8.69-8.27-15.72-5.71"/><radialGradient id="notoBaguetteBread3" cx="47.558" cy="72.772" r="13.1" gradientTransform="matrix(.7602 -.7098 .6856 .612 -37.33 62.34)" gradientUnits="userSpaceOnUse"><stop offset=".188" stop-color="#FBD8A9"/><stop offset=".977" stop-color="#FBD8A9" stop-opacity="0"/></radialGradient><path fill="url(#notoBaguetteBread3)" d="M49.76 60.67c-4.82.44-8.87 1.46-13.25 8.08c-5.79 8.75.17 17.01 8.23 17.38c7.66.35 15.52-4.16 17.5-11.86c2.2-8.6-5.25-14.26-12.48-13.6"/><radialGradient id="notoBaguetteBread4" cx="24.16" cy="100.109" r="16.826" gradientTransform="matrix(.7432 -.6769 .383 .4125 -31.988 75.173)" gradientUnits="userSpaceOnUse"><stop offset="0" stop-color="#FDE5BF"/><stop offset="1" stop-color="#FDE5BF" stop-opacity="0"/></radialGradient><path fill="url(#notoBaguetteBread4)" d="M28.04 86.6c-6.54 1.62-12.19 5.8-15.9 11.84c-5.48 8.92-3.94 14.56.96 16.33c4.9 1.78 19.65-5.34 25.09-15.01c6.45-11.48-2.02-15.18-10.15-13.16"/><path fill="#CE701B" d="M65.95 37.97c-.6-1.2 2.48-4.16 6.37-7.51c3.89-3.36 11.76-8.56 18.65-10.17c7.6-1.77 17.24-1.94 17.42-.09c.14 1.42-6.98 5.75-6.98 5.75L71.34 37.52s-4.86 1.51-5.39.45"/><path fill="#FDCE8C" d="M86.2 27c-11.16 4.85-17.28 11.1-17.28 11.1s10.03.18 23.69-5.04c7.38-2.82 14.57-11.19 14.57-11.19s-10.2.45-20.98 5.13"/><path fill="#CE701B" d="M44.34 66.78s-4.51 2.34-5.64 1.93c-1.13-.4 0-2.18 1.29-4.19c.93-1.45 6.64-8.4 12.96-12.2c6.69-4.03 13.33-5.51 19.77-6.01c5.83-.46 12.97.01 11.61 1.69c-1.5 1.84-3.11 3.65-3.11 3.65z"/><path fill="#FDCE8C" d="M58.45 54.53c9.07-3.73 12.65-3.95 16.85-4.67c4.19-.73 7.9-.48 7.9-.48s-2.2 2.71-4.86 5.21s-6.01 5.06-7.62 5.54s-10.89 3.15-17.19 4.6c-5.32 1.22-12.65 3.6-12.65 3.6s7.98-9.85 17.57-13.8"/><path fill="#CE701B" d="M15.82 96.17c-.19-1.15 6.06-10.66 15.08-15.52s19.85-4.61 22.57-4.55s5.26.35 5.09 1.56s-1.45 1.85-1.45 1.85L17.85 96.58s-1.85.64-2.03-.41"/><path fill="#FDCE8C" d="M27.8 87.21c7.29-5.21 15.15-6.65 20.7-7.34c5.74-.72 8.62-.4 8.62-.4s-3.95 4.4-6.08 6.13c-1.94 1.58-5.74 4.54-7.51 5.26c-1.22.5-8.79 2.26-12.09 3.01s-13.82 2.78-13.82 2.78s1.93-3.55 10.18-9.44"/></symbol><use href="#ai:noto:baguette-bread"></use>  </svg> </div> </a> </section> <p>In the previous article, we explored what enabled the rise of generative AI, along with a brief overview of how AI works, including concepts like learning, weights/parameters, and the importance of turning to Open Source to access lighter models. In this section, we will continue reflecting on the criteria for choosing a model to perform our task and setting up our work environment.</p>
<p>If you’re more interested in the experimental aspect or have limited time, I invite you to go directly to the next article.</p>
<h1 id="first-constraint---the-task-to-perform">First Constraint - The Task to Perform</h1>
<p>It may seem obvious, but choosing a model and its computational power primarily depends on the task at hand.</p>
<div><h3 id="brief-description">Brief Description</h3><p>In this experiment, we will provide a corpus of verbatim responses to our model. Its task: analyze each verbatim entry to identify key ideas. Its objective is to compare the detected ideas with a predefined coding scheme (e.g., Code 1: “The agent was friendly,” Code 2: “The hours are convenient,” etc.).
For each verbatim, the AI will receive detailed instructions in the form of a prompt, including the coding plan and examples associated with each code.</p></div>
<p>Essentially, what the model has to handle is relatively simple, so it is not necessarily required to choose a highly sophisticated model. At first glance, Llama or Mistral models seem to be good candidates.</p>
<h1 id="second-constraint---physical-limitations-of-our-machine">Second Constraint - Physical Limitations of Our Machine</h1>
<div><h3 id="determining-the-available-memory-for-model-computation">Determining the Available Memory for Model Computation</h3><p>In practice, there are two possible options to run AI model computations:</p><ul>
<li>Random Access Memory (RAM) &amp; Central Processing Unit (CPU), available on all computers: the model is loaded into RAM, and calculations are performed by the CPU.</li>
<li>A dedicated Graphics Processing Unit (GPU), found in specific computers (e.g., gaming PCs) or with manually added GPUs: the model is loaded into the GPU’s dedicated memory (VRAM), and computations are executed in parallel within the GPU.</li>
</ul></div>
<p>Personally, I do not have a GPU. This imposes an initial constraint: the capacity of my RAM to load the model. I will therefore rely exclusively on my CPU and RAM to run the model, meaning my RAM must be larger than the model’s size—otherwise, it will crash.</p>
<div><p>To determine your available RAM, you can use the following commands:</p><ul>
<li><strong>On Mac</strong>: Open your terminal and type: <code>system_profiler SPHardwareDataType | grep &quot;Memory&quot;</code></li>
<li><strong>On Linux</strong>: Open your terminal and type: <code>free -g</code></li>
<li><strong>On Windows</strong>: Open your command prompt and type: <code>wmic MEMORYCHIP get Capacity</code></li>
</ul></div>
<div><p>I have 16GB of RAM, which limits the selection of pre-trained models I can use locally. Some viable options include:</p><ul>
<li><strong>LLaMA</strong> from Meta in 7B and 13B versions (7 and 13 billion parameters).</li>
<li><strong>Mistral AI</strong>, which offers equivalent models in 7B and 13B.</li>
<li><strong>Phi-2</strong>, a lighter model with 3 billion parameters.</li>
</ul></div>
<p>However, simply downloading these models is not enough: a full Mistral 7B model weighs around 30GB, far exceeding my RAM capacity (16GB). Therefore, I need an even lighter model to run it locally. Fortunately, there are already ways to work around these limitations.</p>
<h2 id="turning-to-compressed-models-in-gptqgguf-format">Turning to Compressed Models in GPTQ/GGUF Format</h2>
<div><p>We recall that a Large Language Model (LLM) does not store words directly but represents them as tokens, which are converted into lists of numbers (also called vectors). The entire expertise of a model lies in the connections between these lists, known as weights or parameters. These connections are learned during the model’s training and allow it to predict which word or phrase makes the most sense in a given context. Thus, when you see 13 billion parameters (or 13 billion weights), it means there are 13 billion small rules influencing how the model associates words together. The more parameters there are, the better the model can capture complex language nuances, but it also becomes more resource-intensive.</p><div><p>To make models more accessible, compressed formats have been developed: these are known as quantized models, available in formats such as GPTQ and GGUF. How does it work? These models reduce the precision of weights to decrease file size. Specifically, an LLM weight, which is initially stored as a highly precise number (e.g., 0.8463923792834, a 32-bit format), can be rounded and stored as a more compact integer (e.g., 0.8, a 4-bit format). This compression significantly reduces file sizes.</p></div><div><h3 id="example">Example:</h3><ul>
<li>A <strong>Mistral 7B model in 32-bit</strong> weighs <strong>30GB</strong></li>
<li>A <strong>Mistral 7B model in 4-bit</strong> reduces to <strong>6-7GB</strong></li>
</ul></div></div>
<p>Thanks to this size reduction, it becomes possible to run these models locally without exceeding my RAM capacity. This is why quantized models are favored for computers with limited resources.</p>
<p>Now that we know the hardware constraints and suitable models, we can move on to the next step: downloading and running a model locally.</p>
<p>Here, things get technical.</p>
<h1 id="downloading-text-generation-webui-and-the-model-via-huggingface">Downloading Text Generation WebUI and the Model via HuggingFace</h1>
<div><h3 id="downloading-text-generation-webui-and-running-it-locally">Downloading Text Generation WebUI and Running it Locally</h3><p>There are multiple ways to do this. One of the most convenient methods is using Text Generation WebUI, a local front-end interface that simplifies the installation and usage of AI models. This application allows users to manage models, run them, and test their performance without needing to manipulate complex code directly.</p></div>
<div><p>To proceed, we will clone their GitHub repository and install the required dependencies for the project (this may require installing Python and pip):</p><pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8;overflow-x:auto" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#B392F0">git</span><span style="color:#9ECBFF"> clone</span><span style="color:#9ECBFF"> https://github.com/oobabooga/text-generation-webui.git</span></span>
<span class="line"><span style="color:#79B8FF">cd</span><span style="color:#9ECBFF"> text-generation-webui</span></span>
<span class="line"><span style="color:#B392F0">pip</span><span style="color:#9ECBFF"> install</span><span style="color:#79B8FF"> -r</span><span style="color:#9ECBFF"> requirements.txt</span></span>
<span class="line"></span></code></pre></div>
<div><p>During installation, conflicts may arise due to multiple Python projects on your machine, so creating a dedicated environment for this project (via Conda or UV) might be necessary:</p><pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8;overflow-x:auto" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#B392F0">conda</span><span style="color:#9ECBFF"> create</span><span style="color:#79B8FF"> -n</span><span style="color:#9ECBFF"> tgwebui</span><span style="color:#9ECBFF"> python=</span><span style="color:#79B8FF">3.10</span></span>
<span class="line"><span style="color:#B392F0">conda</span><span style="color:#9ECBFF"> activate</span><span style="color:#9ECBFF"> tgwebui</span></span>
<span class="line"></span></code></pre></div>
<div><p>Then, you can restart the installation (<code>pip install -r requirements.txt</code>).</p></div>
<div><p>Next, depending on your machine, you will need to run the launch script. On my MacBook, I need to enter <code>./start_macos.sh</code> in my terminal to start the application. Alternatively, the application can be launched by executing the <code>server.py</code> file in the terminal:</p><pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8;overflow-x:auto" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#B392F0">python</span><span style="color:#9ECBFF"> server.py</span><span style="color:#79B8FF"> --api</span><span style="color:#79B8FF"> --extensions</span><span style="color:#9ECBFF"> api,openai</span></span>
<span class="line"></span></code></pre></div>
<div><p>This process takes some time, and the terminal will display the following:</p><pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8;overflow-x:auto" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#E1E4E8">(</span><span style="color:#B392F0">tgwebui</span><span style="color:#E1E4E8">) </span><span style="color:#B392F0">➜</span><span style="color:#9ECBFF">  text-generation-webui</span><span style="color:#9ECBFF"> git:</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">main</span><span style="color:#E1E4E8">) </span><span style="color:#9ECBFF">✗</span><span style="color:#9ECBFF"> python</span><span style="color:#9ECBFF"> server.py</span><span style="color:#79B8FF"> --api</span><span style="color:#79B8FF"> --extensions</span><span style="color:#9ECBFF"> api,openai</span><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#B392F0">14:39:56-175066</span><span style="color:#9ECBFF"> INFO</span><span style="color:#9ECBFF">     Starting</span><span style="color:#9ECBFF"> Text</span><span style="color:#9ECBFF"> generation</span><span style="color:#9ECBFF"> web</span><span style="color:#9ECBFF"> UI</span><span style="color:#E1E4E8">                                                                                      </span></span>
<span class="line"><span style="color:#B392F0">14:39:56-177648</span><span style="color:#9ECBFF"> INFO</span><span style="color:#9ECBFF">     Loading</span><span style="color:#9ECBFF"> the</span><span style="color:#9ECBFF"> extension</span><span style="color:#9ECBFF"> &quot;openai&quot;</span><span style="color:#E1E4E8">                                                                                       </span></span>
<span class="line"><span style="color:#B392F0">14:39:56-807749</span><span style="color:#9ECBFF"> INFO</span><span style="color:#9ECBFF">     OpenAI-compatible</span><span style="color:#9ECBFF"> API</span><span style="color:#9ECBFF"> URL:</span><span style="color:#E1E4E8">                                                                                            </span></span>
<span class="line"><span style="color:#E1E4E8">                                                                                                                                              </span></span>
<span class="line"><span style="color:#B392F0">                         http://127.0.0.1:5000</span><span style="color:#E1E4E8">                                                                                                 </span></span>
<span class="line"><span style="color:#E1E4E8">                                                                                                                                              </span></span>
<span class="line"><span style="color:#B392F0">Running</span><span style="color:#9ECBFF"> on</span><span style="color:#9ECBFF"> local</span><span style="color:#9ECBFF"> URL:</span><span style="color:#9ECBFF">  http://127.0.0.1:7860</span></span>
<span class="line"></span></code></pre></div>
<div><p>What interests us are the addresses displayed at the bottom (to view the application in a browser: <a href="http://127.0.0.1:7860">http://127.0.0.1:7860</a>). Following this link reveals various tabs (chat, default, notebook, etc.). However, at this stage, no AI model has been downloaded, so interaction with the chat is not yet possible.</p></div>
<div><p>To proceed, go to the “Model” tab and locate the “Download” section on the right. In the field labeled “Download model or LoRA,” enter the name of a model. To find your model, we will turn to HuggingFace.</p></div>
<h2 id="huggingface">HuggingFace</h2>
<div><h3 id="origins">Origins</h3><p>HuggingFace is a French unicorn company (cocorico). Its uniqueness? An open and community-driven approach: according to its founders, AI models are too important to be monopolized by private companies. They have therefore created an open-source hub where anyone can share, download, and experiment with AI models for free.
This principle is ideal for our project as it allows us to retrieve a model suited to our machine and needs.</p></div>
<div><h3 id="downloading-a-model-via-huggingface">Downloading a Model via HuggingFace</h3><p>As mentioned earlier, we need a quantized model in GGUF format, specifically Mistral 7B / Llama 7B. To do this, we will:</p><ol>
<li>Use HuggingFace’s search engine to look for Mistral 7B GGUF models.</li>
<li>Choose the most popular model. The search yields numerous results, but to simplify, we will select the most downloaded model. Our choice: MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF (1.4 million downloads last month).</li>
<li>Start the download in Text Generation WebUI:
<ul>
<li>Open Text Generation WebUI locally (for me, it’s: <a href="http://127.0.0.1:7860">http://127.0.0.1:7860</a>, as seen above).</li>
<li>Go to the “Download” tab.</li>
<li>Enter MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF in the corresponding field.</li>
</ul>
</li>
<li>While we can download everything, we will limit ourselves to Q4 files (4-bit quantization), which offer the best balance between size and performance. In the file download field, enter:
<ul>
<li>Q4_K_M (download this file first).</li>
<li>These files are large (3 to 6GB, so download time depends on your connection speed).</li>
<li>Once finished, enter Q4_K_S and start the second download.</li>
</ul>
</li>
</ol></div>
<div><h3 id="loading-the-model-in-webui">Loading the Model in WebUI</h3><p>Once the download(s) are complete:</p><ol>
<li>Refresh the interface:
<ul>
<li>Click on the refresh icon to update the list of available models on the left.</li>
<li>If successful, the downloaded models will appear in the list.</li>
</ul>
</li>
<li>Change the Model Loader:
<ul>
<li>By default, the Model Loader is set to Transformers.</li>
<li>We need to change it to Llama.cpp (suitable for GGUF models).</li>
</ul>
</li>
<li>Load the model:
<ul>
<li>Click “Load.”</li>
</ul>
</li>
</ol></div>
<div><p>If the loading fails, an error message appears instead.
Example:</p><pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8;overflow-x:auto" tabindex="0" data-language="plaintext"><code><span class="line"><span>Llama = llama_cpp_lib().Llama</span></span>
<span class="line"><span>AttributeError: &#39;NoneType&#39; object has no attribute &#39;Llama&#39;</span></span>
<span class="line"><span></span></span></code></pre></div>
<p>For me, this issue was caused by <code>llama-cpp-python</code> not being installed in my execution environment. I had to install it using <code>pip install llama-cpp-python</code> within my Conda environment.</p>
<div><p>If everything works, you can now go to the chat tab and interact with the model directly within the interface.</p></div>
<div><p>If all is well, we can move on to the next step: starting to automate our coding process. (To be continued in the next article…)</p></div> </article>   <style>astro-island,astro-slot,astro-static-slot{display:contents}</style><script>(()=>{var e=async t=>{await(await t())()};(self.Astro||(self.Astro={})).load=e;window.dispatchEvent(new Event("astro:load"));})();;(()=>{var A=Object.defineProperty;var g=(i,o,a)=>o in i?A(i,o,{enumerable:!0,configurable:!0,writable:!0,value:a}):i[o]=a;var d=(i,o,a)=>g(i,typeof o!="symbol"?o+"":o,a);{let i={0:t=>m(t),1:t=>a(t),2:t=>new RegExp(t),3:t=>new Date(t),4:t=>new Map(a(t)),5:t=>new Set(a(t)),6:t=>BigInt(t),7:t=>new URL(t),8:t=>new Uint8Array(t),9:t=>new Uint16Array(t),10:t=>new Uint32Array(t),11:t=>1/0*t},o=t=>{let[l,e]=t;return l in i?i[l](e):void 0},a=t=>t.map(o),m=t=>typeof t!="object"||t===null?t:Object.fromEntries(Object.entries(t).map(([l,e])=>[l,o(e)]));class y extends HTMLElement{constructor(){super(...arguments);d(this,"Component");d(this,"hydrator");d(this,"hydrate",async()=>{var b;if(!this.hydrator||!this.isConnected)return;let e=(b=this.parentElement)==null?void 0:b.closest("astro-island[ssr]");if(e){e.addEventListener("astro:hydrate",this.hydrate,{once:!0});return}let c=this.querySelectorAll("astro-slot"),n={},h=this.querySelectorAll("template[data-astro-template]");for(let r of h){let s=r.closest(this.tagName);s!=null&&s.isSameNode(this)&&(n[r.getAttribute("data-astro-template")||"default"]=r.innerHTML,r.remove())}for(let r of c){let s=r.closest(this.tagName);s!=null&&s.isSameNode(this)&&(n[r.getAttribute("name")||"default"]=r.innerHTML)}let p;try{p=this.hasAttribute("props")?m(JSON.parse(this.getAttribute("props"))):{}}catch(r){let s=this.getAttribute("component-url")||"<unknown>",v=this.getAttribute("component-export");throw v&&(s+=` (export ${v})`),console.error(`[hydrate] Error parsing props for component ${s}`,this.getAttribute("props"),r),r}let u;await this.hydrator(this)(this.Component,p,n,{client:this.getAttribute("client")}),this.removeAttribute("ssr"),this.dispatchEvent(new CustomEvent("astro:hydrate"))});d(this,"unmount",()=>{this.isConnected||this.dispatchEvent(new CustomEvent("astro:unmount"))})}disconnectedCallback(){document.removeEventListener("astro:after-swap",this.unmount),document.addEventListener("astro:after-swap",this.unmount,{once:!0})}connectedCallback(){if(!this.hasAttribute("await-children")||document.readyState==="interactive"||document.readyState==="complete")this.childrenConnectedCallback();else{let e=()=>{document.removeEventListener("DOMContentLoaded",e),c.disconnect(),this.childrenConnectedCallback()},c=new MutationObserver(()=>{var n;((n=this.lastChild)==null?void 0:n.nodeType)===Node.COMMENT_NODE&&this.lastChild.nodeValue==="astro:end"&&(this.lastChild.remove(),e())});c.observe(this,{childList:!0}),document.addEventListener("DOMContentLoaded",e)}}async childrenConnectedCallback(){let e=this.getAttribute("before-hydration-url");e&&await import(e),this.start()}async start(){let e=JSON.parse(this.getAttribute("opts")),c=this.getAttribute("client");if(Astro[c]===void 0){window.addEventListener(`astro:${c}`,()=>this.start(),{once:!0});return}try{await Astro[c](async()=>{let n=this.getAttribute("renderer-url"),[h,{default:p}]=await Promise.all([import(this.getAttribute("component-url")),n?import(n):()=>()=>{}]),u=this.getAttribute("component-export")||"default";if(!u.includes("."))this.Component=h[u];else{this.Component=h;for(let f of u.split("."))this.Component=this.Component[f]}return this.hydrator=p,this.hydrate},e,this)}catch(n){console.error(`[astro-island] Error hydrating ${this.getAttribute("component-url")}`,n)}}attributeChangedCallback(){this.hydrate()}}d(y,"observedAttributes",["props"]),customElements.get("astro-island")||customElements.define("astro-island",y)}})();</script><astro-island uid="1R5q0b" prefix="r0" component-url="/_astro/NavBarContainer.SLe6ONYC.js" component-export="default" renderer-url="/_astro/client.BY2mA-CD.js" props="{&quot;imageUrl&quot;:[0,&quot;/_astro/Logo-blanc.BpWAmt-s_1SKg5T.webp&quot;]}" ssr="" client="load" opts="{&quot;name&quot;:&quot;NavBarContainer&quot;,&quot;value&quot;:true}" await-children=""><div><aside class="sidebar-container closed"><div class="icon"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 352 512" class="close-icon" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M242.72 256l100.07-100.07c12.28-12.28 12.28-32.19 0-44.48l-22.24-22.24c-12.28-12.28-32.19-12.28-44.48 0L176 189.28 75.93 89.21c-12.28-12.28-32.19-12.28-44.48 0L9.21 111.45c-12.28 12.28-12.28 32.19 0 44.48L109.28 256 9.21 356.07c-12.28 12.28-12.28 32.19 0 44.48l22.24 22.24c12.28 12.28 32.2 12.28 44.48 0L176 322.72l100.07 100.07c12.28 12.28 32.2 12.28 44.48 0l22.24-22.24c12.28-12.28 12.28-32.19 0-44.48L242.72 256z"></path></svg></div><div class="sidebar-wrapper"><ul class="sidebar-menu"><li><a href="/" class="sidebar-link">Blog</a></li><li><a href="/about" class="sidebar-link">About</a></li><li><a href="/projects" class="sidebar-link">Projects</a></li><li><a href="/contact" class="sidebar-link">Contact</a></li></ul><div class="side-btn-wrap"></div></div></aside><div class="nav"><div class="navbar-container"><div class="nav-logo"><a href="/"><img src="/_astro/Logo-blanc.BpWAmt-s_1SKg5T.webp" alt="logo"/></a></div><div class="mobile-icon"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" class="burger" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M16 132h416c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163 0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"></path></svg></div><div class="nav-menu"><div class="nav-items"><a href="/" class="nav-links"><span class="number">1. </span> Blog</a><a href="/about" class="nav-links"><span class="number">2. </span> About</a><a href="/projects" class="nav-links"><span class="number">3. </span> Projects</a><a href="/contact" class="nav-links"><span class="number">4. </span> Contact</a></div></div><div class="resume-container"><a href="/CV/CV_Romain_Basly_feb2025_Chef_projet_PO.pdf" download=""><button class="download-resume">Resume</button></a></div></div></div></div><!--astro:end--></astro-island> <div class="logo-container"><a target="_blank" rel="noopener noreferrer noreferrer" href="https://calendly.com/romain-basly/first-contact"><svg xmlns="http://www.w3.org/2000/svg" fill="#000000" version="1.1" id="Capa_1" width="800px" height="800px" viewBox="0 0 612 612" class="logo-calendly"><g><g><path d="M612,463.781c0-70.342-49.018-129.199-114.75-144.379c-10.763-2.482-21.951-3.84-33.469-3.84    c-3.218,0-6.397,0.139-9.562,0.34c-71.829,4.58-129.725,60.291-137.69,131.145c-0.617,5.494-0.966,11.073-0.966,16.734    c0,10.662,1.152,21.052,3.289,31.078C333.139,561.792,392.584,612,463.781,612C545.641,612,612,545.641,612,463.781z     M463.781,561.797c-54.133,0-98.016-43.883-98.016-98.016s43.883-98.016,98.016-98.016s98.016,43.883,98.016,98.016    S517.914,561.797,463.781,561.797z"></path><polygon points="482.906,396.844 449.438,396.844 449.438,449.438 396.844,449.438 396.844,482.906 482.906,482.906     482.906,449.438 482.906,449.438   "></polygon><path d="M109.969,0c-9.228,0-16.734,7.507-16.734,16.734v38.25v40.641c0,9.228,7.506,16.734,16.734,16.734h14.344    c9.228,0,16.734-7.507,16.734-16.734V54.984v-38.25C141.047,7.507,133.541,0,124.312,0H109.969z"></path><path d="M372.938,0c-9.228,0-16.734,7.507-16.734,16.734v38.25v40.641c0,9.228,7.507,16.734,16.734,16.734h14.344    c9.228,0,16.734-7.507,16.734-16.734V54.984v-38.25C404.016,7.507,396.509,0,387.281,0H372.938z"></path><path d="M38.25,494.859h236.672c-2.333-11.6-3.572-23.586-3.572-35.859c0-4.021,0.177-7.999,0.435-11.953H71.719    c-15.845,0-28.688-12.843-28.688-28.688v-229.5h411.188v88.707c3.165-0.163,6.354-0.253,9.562-0.253    c11.437,0,22.61,1.109,33.469,3.141V93.234c0-21.124-17.126-38.25-38.25-38.25h-31.078v40.641c0,22.41-18.23,40.641-40.641,40.641    h-14.344c-22.41,0-40.641-18.231-40.641-40.641V54.984H164.953v40.641c0,22.41-18.231,40.641-40.641,40.641h-14.344    c-22.41,0-40.641-18.231-40.641-40.641V54.984H38.25C17.126,54.984,0,72.111,0,93.234v363.375    C0,477.733,17.126,494.859,38.25,494.859z"></path><circle cx="134.774" cy="260.578" r="37.954"></circle><circle cx="248.625" cy="260.578" r="37.954"></circle><circle cx="362.477" cy="260.578" r="37.954"></circle><circle cx="248.625" cy="375.328" r="37.953"></circle><circle cx="134.774" cy="375.328" r="37.953"></circle></g></g></svg></a><a target="_blank" rel="noopener noreferrer noreferrer" href="https://github.com/RomainBasly"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" class="logo-github" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a target="_blank" rel="noopener noreferrer noreferrer" href="http://www.linkedin.com/in/romainbasly"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" class="logo-linkedin" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M100.28 448H7.4V148.9h92.88zM53.79 108.1C24.09 108.1 0 83.5 0 53.8a53.79 53.79 0 0 1 107.58 0c0 29.7-24.1 54.3-53.79 54.3zM447.9 448h-92.68V302.4c0-34.7-.7-79.2-48.29-79.2-48.29 0-55.69 37.7-55.69 76.7V448h-92.78V148.9h89.08v40.8h1.3c12.4-23.5 42.69-48.3 87.88-48.3 94 0 111.28 61.9 111.28 142.3V448z"></path></svg></a><div class="line-socials"></div></div> <div class="container-email"><a href="mailto:romain.basly@protonmail.com" class="email">romain.basly@protonmail.com</a><div class="line-email"></div></div> </body></html>